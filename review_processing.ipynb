{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/debasmitroy/Desktop/adv_rag/ai_backend/myenv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import phoenix as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    }
   ],
   "source": [
    "import llama_index\n",
    "from llama_index.core import Document\n",
    "from llama_index.indices.managed.vectara import VectaraIndex,VectaraAutoRetriever\n",
    "from llama_index.core.schema import TextNode\n",
    "from llama_index.core.vector_stores import MetadataInfo, VectorStoreInfo\n",
    "from llama_index.core.indices.service_context import ServiceContext\n",
    "from llama_index.llms.together import TogetherLLM\n",
    "\n",
    "from common_imports import *\n",
    "import json , os , sys , time , re\n",
    "import dotenv\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üåç To view the Phoenix app in your browser, visit http://localhost:6006/\n",
      "üì∫ To view the Phoenix app in a notebook, run `px.active_session().view()`\n",
      "üìñ For more information on how to use Phoenix, check out https://docs.arize.com/phoenix\n"
     ]
    }
   ],
   "source": [
    "px.launch_app()\n",
    "llama_index.core.set_global_handler(\"arize_phoenix\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review Emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_reviews_into_textnodes(reviews):\n",
    "    # reviews : list of reviews \n",
    "    # review_documents = [Document(text=review['text'],metadata=review , doc_id=review['metadata']) for review in reviews]\n",
    "    review_nodes = [TextNode(text=review['text'],metadata={\n",
    "        'source':str(review['source']),\n",
    "        'date_time':str(review['metadata']['at']),\n",
    "        'reviewId':str(review['metadata']['reviewId']),\n",
    "        'userName':str(review['metadata']['userName']),\n",
    "        'rating':str(review['metadata']['score']),\n",
    "        'thumbsUpCount':str(review['metadata']['thumbsUpCount']),\n",
    "        'appVersion':str(review['metadata']['appVersion']),\n",
    "        'replyContent':str(review['metadata']['replyContent']),\n",
    "        'repliedAt':str(review['metadata']['repliedAt']),\n",
    "        'text':str(review['text']),\n",
    "        'app_name':str(review['app_name']),\n",
    "    },id_ = review['metadata']['reviewId']\n",
    "    ) for review in reviews]\n",
    "\n",
    "    return review_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reviews_from_file(file_path):\n",
    "    # file_path : path to the file containing reviews\n",
    "    # Returns list of reviews\n",
    "    with open(file_path,'r') as f:\n",
    "        reviews = json.load(f)\n",
    "    return reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReviewVectaraEngine:\n",
    "    def __init__(\n",
    "            self,\n",
    "            verbose=True, \n",
    "            similarity_top_k=2,\n",
    "            summary_enabled=False,\n",
    "            summary_response_lang=\"eng\",\n",
    "            summary_num_results=7,\n",
    "            llm_model_name=\"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n",
    "            ):\n",
    "        self.index = VectaraIndex(show_progress=True)\n",
    "        self.vector_store_info = VectorStoreInfo(\n",
    "            content_info = \"App reviews from different sources\",\n",
    "            metadata_info = [\n",
    "                MetadataInfo(\n",
    "                    name = \"source\",\n",
    "                    type = \"string\",\n",
    "                    description=\"Source of the review like playstore,appstore etc\"\n",
    "                ),\n",
    "                MetadataInfo(\n",
    "                    name = \"date_time\",\n",
    "                    type = \"string\",\n",
    "                    description=\"Date and time of the review\"\n",
    "                ),\n",
    "                MetadataInfo(\n",
    "                    name = \"reviewId\",\n",
    "                    type = \"string\",\n",
    "                    description=\"Review Id\"\n",
    "                ),\n",
    "                MetadataInfo(\n",
    "                    name = \"userName\",\n",
    "                    type = \"string\",\n",
    "                    description=\"User name of the reviewer\"\n",
    "                ),\n",
    "                MetadataInfo(\n",
    "                    name = \"rating\",\n",
    "                    type = \"float\",\n",
    "                    description=\"Rating given by the reviewer\"\n",
    "                ),\n",
    "                MetadataInfo(\n",
    "                    name = \"thumbsUpCount\",\n",
    "                    type = \"int\",\n",
    "                    description=\"Number of thumbs up i.e. the relevance of the review\"\n",
    "                ),\n",
    "                MetadataInfo(\n",
    "                    name = \"appVersion\",\n",
    "                    type = \"string\",\n",
    "                    description=\"App version of the app for which review is given\"\n",
    "                ),\n",
    "                MetadataInfo(\n",
    "                    name = \"replyContent\",\n",
    "                    type = \"string\",\n",
    "                    description=\"Reply content to the review by any other user\"\n",
    "                ),\n",
    "                MetadataInfo(\n",
    "                    name = \"repliedAt\",\n",
    "                    type = \"string\",\n",
    "                    description=\"Date and time of the reply\"\n",
    "                ),\n",
    "                MetadataInfo(\n",
    "                    name = \"app_name\",\n",
    "                    type = \"string\",\n",
    "                    description=\"App name for which review is given\"\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "        self.llm = TogetherLLM(\n",
    "            model=llm_model_name, api_key=os.environ['TOGETHER_API_KEY']\n",
    "        )\n",
    "\n",
    "        self.verbose = verbose\n",
    "        self.similarity_top_k = similarity_top_k\n",
    "        self.summary_enabled = summary_enabled\n",
    "        self.summary_response_lang = summary_response_lang\n",
    "        self.summary_num_results = summary_num_results\n",
    "\n",
    "        self.build()\n",
    "    \n",
    "    def build(self):\n",
    "        self.auto_retriever = VectaraAutoRetriever(\n",
    "            vector_store_info=self.vector_store_info,\n",
    "            llm=self.llm,\n",
    "            index=self.index,\n",
    "            show_progress=True,\n",
    "            summary_enabled = self.summary_enabled,\n",
    "            summary_response_lang = self.summary_response_lang,\n",
    "            summary_num_results = self.summary_num_results,\n",
    "            verbose=self.verbose,\n",
    "        )\n",
    "        self.retriever = self.index.as_retriever(\n",
    "            similarity_top_k=self.similarity_top_k,\n",
    "            summary_enabled = self.summary_enabled,\n",
    "            summary_response_lang = self.summary_response_lang,\n",
    "            summary_num_results = self.summary_num_results,\n",
    "            llm = self.llm,\n",
    "            )\n",
    "        self.query_engine = self.index.as_query_engine(similarity_top_k=5)\n",
    "\n",
    "    def ingest_reviews(self,review_file_path,start=None,end=None):\n",
    "        self.reviews = get_reviews_from_file(review_file_path)[start:end]\n",
    "        self.review_nodes = convert_reviews_into_textnodes(self.reviews)\n",
    "        self.index = VectaraIndex(nodes = self.review_nodes, show_progress=True)\n",
    "        self.build()\n",
    "    \n",
    "    def ingest_nodes(self,nodes):\n",
    "        self.index = VectaraIndex(nodes = nodes, show_progress=True)\n",
    "        self.build()\n",
    "\n",
    "    def run(self,query , mode:str):\n",
    "        # query : query string\n",
    "        # mode : 'autoretriever' or 'retriever' or 'query_engine'\n",
    "        if mode == 'autoretriever':\n",
    "            return self.auto_retriever.retrieve(query)\n",
    "        elif mode == 'retriever':\n",
    "            return self.retriever.retrieve(query)\n",
    "        elif mode == 'query_engine':\n",
    "            return self.query_engine.query(query)\n",
    "        else:\n",
    "            return \"Invalid mode\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _review_file_path = './datas/api_result_reviews_relv_Google_Pay_Secure_UPI_payment_v0.json'\n",
    "# _reviews = get_reviews_from_file(_review_file_path)\n",
    "# _review_nodes = convert_reviews_into_textnodes(_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _review_nodes[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# review_vectara_engine = ReviewVectaraEngine(\n",
    "#     verbose=True, \n",
    "#     similarity_top_k=2,\n",
    "#     summary_enabled=False,\n",
    "#     summary_response_lang=\"eng\",\n",
    "#     summary_num_results=7,\n",
    "#     llm_model_name=\"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# review_file_path = './datas/api_result_reviews_relv_Grand_Theft_Auto_San_Andreas_v0.json'\n",
    "# review_engine.ingest_reviews(review_file_path,end=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ans = review_engine.run('What are the reviews from playstore',mode='autoretriever')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generic JSON Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "JSON_Engine is a wrapper over LlamaIndex JSON Query Engine\n",
    "It takes a JSON prompt and a pydanctic class name as input in the constructor\n",
    "It uses OpenAI API to generate the output\n",
    "\n",
    "run method processes the prompt and returns the output in the form of pydantic class object\n",
    "\"\"\"\n",
    "class JSON_Engine(BaseTool):\n",
    "    def __init__(self, prompt, class_name, llm_model_name: str = \"mistralai/Mixtral-8x7B-Instruct-v0.1\",temperature=0.1, api_key_name = \"TOGETHER_API_KEY\",parse=True,gemma_mode=False):\n",
    "        self.output_parser = PydanticOutputParser(class_name)\n",
    "        # self.llm = TogetherLLM(model=llm_model_name, api_key=os.environ[api_key_name], temperature=temperature)\n",
    "        self.llm = OpenAI(model=\"gpt-3.5-turbo\", temperature=temperature)\n",
    "        self.json_prompt_str = prompt\n",
    "        self.class_name = class_name\n",
    "        self.json_prompt_str = self.output_parser.format(self.json_prompt_str)\n",
    "        self.json_prompt_tmpl = PromptTemplate(self.json_prompt_str)\n",
    "        if parse:\n",
    "            self.p = QueryPipeline(chain=[self.json_prompt_tmpl, self.llm, self.output_parser], verbose=False)\n",
    "        else:\n",
    "            self.p = QueryPipeline(chain=[self.json_prompt_tmpl, self.llm], verbose=False)\n",
    "        self.gemma_mode = gemma_mode\n",
    "    \n",
    "    def run(self, **kwargs):\n",
    "        response = self.p.run(**kwargs)\n",
    "        if self.gemma_mode:\n",
    "            pattern = r'{.*}'\n",
    "            matches = re.findall(pattern, response.message.content, re.DOTALL)\n",
    "            largest_match = max(matches, key=len, default=None)\n",
    "            print(largest_match)\n",
    "            response = self.output_parser.parse(largest_match)\n",
    "        return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Ingestion Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Isssue(BaseModel):\n",
    "    issue:str = Field(...,description=\"\"\"\n",
    "    A negative aspect extracted from the review context. It should be a full sentence and complete.\n",
    "    \"\"\")\n",
    "\n",
    "    containsAnyReport:bool = Field(...,description=\"\"\"\n",
    "    Whether the issue contains any particular report or not. It may user is reporting some issue or bug like any glitch, crash, lag, something not working etc. \n",
    "    True means the issue contains any particular report\n",
    "    False means the issue does not contain any particular report\n",
    "    \"\"\")\n",
    "\n",
    "    featureRequest:bool = Field(...,description=\n",
    "    \"\"\"\n",
    "    Whether the issue is a feature request or not. It may user is unsatisfied with the current features and wants some new features / upgrade.\n",
    "    True means the issue is a feature request\n",
    "    False means the issue is not a feature request\n",
    "    \"\"\")\n",
    "\n",
    "class IssueList(BaseModel):\n",
    "    issues:List[Isssue] = Field(...,description=\n",
    "    \"\"\"\n",
    "    List of different Issues from review context\n",
    "    \"\"\"\n",
    "    )\n",
    "\n",
    "class IssueEngine:\n",
    "    def __init__(self):\n",
    "        self.engine = JSON_Engine(\"\"\"\n",
    "        Given a review context, extract the list of different negative aspects mentioned in the review context:\n",
    "        <<<\n",
    "        {text}\n",
    "        >>>\n",
    "\n",
    "        Note: \n",
    "        These aspects should be all different negative aspects of the review context and should be full sentences.\n",
    "        Also they must be complete i.e. they should not be a part of bigger aspects.\n",
    "        If there are no negative aspects, return an empty list. \n",
    "\n",
    "        \"\"\",class_name = IssueList,temperature=0.2,gemma_mode=False,parse=True)\n",
    "    \n",
    "    def __call__(self,**kwargs):\n",
    "        return self.engine.run(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReviewDegenAndSentimentPipeline:\n",
    "    def __init__(self):\n",
    "        self.issue_engine = IssueEngine()\n",
    "    \n",
    "    def __call__(self,review):\n",
    "        issues = self.issue_engine(text=review)\n",
    "        issues = issues.dict()['issues']\n",
    "        return issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['OPENAI_API_KEY'] = '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'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "_sub_contexts = ReviewDegenAndSentimentPipeline()(review=\"\"\"\n",
    "Hi, After the last update, I'm not able to open the app. When I click on the login id, the UI disappear's and close the application. Kindly fix the issue ASAP. Secondly, the experience was excellent and swift before, but the cash back is very low. Everyone needs cash backs instead of irrelevant vouchers and coupons. Kindly bring these fixes ASAP. Keep growing guys. Thank you google pay.!!\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _sub_contexts.message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regex pattern to match the largest content within curly braces\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(largest_match)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Routes for different team"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UtterenceList(BaseModel):\n",
    "    utterenceList : List[str] = Field(...,description=\"\"\"List of different utterances / use case related to the given team's work\"\"\")\n",
    "\n",
    "class UtterenceEngine:\n",
    "    def __init__(self):\n",
    "        self.engine = JSON_Engine(\"\"\"\n",
    "        Given an app description (between ### and ###):\n",
    "        ###\n",
    "        {app_description}\n",
    "        ###\n",
    "\n",
    "\n",
    "        Given a team's scope of work (between <<< and >>>):\n",
    "        <<<\n",
    "        {scopes}\n",
    "        >>>\n",
    "\n",
    "        List down the more than 15 different utterances / use cases related to the given team's work and the app_description where they work.\n",
    "        These utterances should be full sentences and refers to the different bugs, features, improvements, etc. related to the team's work.\n",
    "        Strictly stick to the scope of work of the team.\n",
    "        \"\"\",class_name = UtterenceList,temperature=0.1)\n",
    "    \n",
    "    def __call__(self,**kwargs):\n",
    "        return self.engine.run(**kwargs)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TechnicalSummary(BaseModel):\n",
    "    summary:str = Field(...,description=\n",
    "            \"\"\"\n",
    "            Summary of all technical details associated with the app\n",
    "            \"\"\")\n",
    "\n",
    "class TechnicalSummaryEngine:\n",
    "    def __init__(self):\n",
    "        self.engine = JSON_Engine(\"\"\"\n",
    "        Given a raw app description (between <<< and >>>):\n",
    "        <<<\n",
    "        {app_description}\n",
    "        >>>\n",
    "        \n",
    "        Summarize all the technical details associated with the app into a brief summary. Only include techincal features. \n",
    "        \"\"\",class_name = TechnicalSummary,temperature=0.1)\n",
    "    \n",
    "    def __call__(self,**kwargs):\n",
    "        return self.engine.run(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TeamAndReview(BaseModel):\n",
    "    team:int = Field(...,description=\n",
    "    \"\"\"\n",
    "    Team's ID\n",
    "    \"\"\")\n",
    "\n",
    "    reason_for_assignment:str = Field(...,description=\n",
    "    \"\"\"\n",
    "    Reason for assigning the team to the review\n",
    "    \"\"\")\n",
    "\n",
    "class TeamAndReviewList(BaseModel):\n",
    "    teams:List[TeamAndReview] = Field(...,description=\n",
    "    \"\"\"\n",
    "    List of different teams assigned to the reviews\n",
    "    \"\"\"\n",
    "    )\n",
    "\n",
    "class TeamAndReviewEngine:\n",
    "    def __init__(self):\n",
    "        self.engine = JSON_Engine(\"\"\"\n",
    "        Given a list of teams along with their IDs and scope of work (between <<< and >>>):\n",
    "        <<<\n",
    "        {teams}\n",
    "        >>>\n",
    "                                  \n",
    "\n",
    "        Given a review context (between ### and ###):\n",
    "        ###\n",
    "        {review}\n",
    "        ###\n",
    "\n",
    "        This review can be assigned to multiple teams based on the context of the review and the scope of work of the teams.\n",
    "        List down the different teams along with their IDs to which the review can be assigned based on the review context.\n",
    "        Also provide the reason for assigning the review to the team. \n",
    "        Strictly stick to the scope of work of the teams and check if the context of the review matches with the scope of work of the teams.\n",
    "        \"\"\",class_name = TeamAndReviewList,temperature=0.1)\n",
    "\n",
    "    def __call__(self,**kwargs):\n",
    "        return self.engine.run(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TeamRoutePipeline:\n",
    "    def __init__(\n",
    "            self, \n",
    "            team_details_file_path:str, \n",
    "            app_details_file_path:str,\n",
    "            app_description=None,\n",
    "            ):\n",
    "        \n",
    "        self.team_details_file_path = team_details_file_path\n",
    "        # load app details from the file\n",
    "        with open(app_details_file_path,'r') as f:\n",
    "            app_details = json.load(f)\n",
    "        with open(self.team_details_file_path,'r') as f:\n",
    "            self.team_details = json.load(f)\n",
    "\n",
    "        self.app_name = app_details[\"title\"]\n",
    "        self.app_description = app_description\n",
    "        # self.utterance_engine = UtterenceEngine()\n",
    "        self.technical_summary_engine = TechnicalSummaryEngine()\n",
    "\n",
    "        self.team_route_engine = TeamAndReviewEngine()\n",
    "        self.team_subprompt = None\n",
    "        \n",
    "        # self.score_threshold_config = score_threshold_config\n",
    "        # if score_threshold_config is None:\n",
    "        #     self.score_threshold_config = {team:score_threshold for team in self.team_details[\"teams\"]}\n",
    "\n",
    "        if app_description is None:\n",
    "            self.app_description = app_details[\"description\"]\n",
    "            self.summarized_app_description = self.technical_summary_engine(app_description=self.app_description).summary\n",
    "        # if build_mode:\n",
    "        #     self.build()\n",
    "\n",
    "    # def build(self):\n",
    "    #     # load team details from the file\n",
    "    #     with open(self.team_details_file_path,'r') as f:\n",
    "    #         self.team_details = json.load(f)\n",
    "    #     print(\"Building utterances ...\")\n",
    "    #     for i,team in tqdm(enumerate(self.team_details[\"teams\"])):\n",
    "    #         print(f\"Building utterances for team : {team['teamName']} ...\")\n",
    "    #         utterance_ans = self.utterance_engine(app_description=self.summarized_app_description,scopes=str(team[\"scopes\"])).utterenceList\n",
    "    #         self.team_details[\"teams\"][i][\"utterances\"] = utterance_ans\n",
    "    #     # save the updated team details to the file\n",
    "    #     with open(self.team_details_file_path,'w') as f:\n",
    "    #         json.dump(self.team_details,f,indent=4)  \n",
    "\n",
    "    def build_routes(self):\n",
    "        with open(self.team_details_file_path,'r') as f:\n",
    "            self.team_details = json.load(f)\n",
    "\n",
    "        print(\"Building route layers ...\")\n",
    "        self.team_subprompt = \"\"\n",
    "        for i,team in tqdm(enumerate(self.team_details[\"teams\"])):\n",
    "            team_name = team[\"teamName\"]\n",
    "            scopes = team[\"scopes\"]\n",
    "            self.team_subprompt += f\"TeamID:{i} , TeamName:{team_name} , Scopes:{scopes}\\n\"\n",
    "\n",
    "\n",
    "\n",
    "    def route_text(self, text):\n",
    "        if self.team_subprompt is None:\n",
    "           self.build_routes()\n",
    "        route_ans =  self.team_route_engine(teams=self.team_subprompt,review=text)\n",
    "        return route_ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReviewRoutePipeline:\n",
    "    def __init__(self, team_route_obj):\n",
    "        self.review_degen_pipeline = ReviewDegenAndSentimentPipeline()\n",
    "        self.team_route_obj = team_route_obj\n",
    "        self.backup = []\n",
    "\n",
    "    def __call__(self,review_nodes):\n",
    "        review_nodes_assigned = []\n",
    "        for review_node in review_nodes:\n",
    "            issues = self.review_degen_pipeline(review_node.text) \n",
    "            # assign each subset to a team\n",
    "            # Loop over subsets_with_sentiment \n",
    "            teams = []\n",
    "            review_node.metadata['assigned_teams'] = []\n",
    "            review_node.metadata['reasons'] = []\n",
    "            review_node.metadata['issues'] = []\n",
    "            # review_node.metadata['positive_keywords'] = []\n",
    "            review_sub_prompt = \"\"\n",
    "            for issue in issues:\n",
    "                if issue[\"containsAnyReport\"] or issue['featureRequest']:\n",
    "                    review_sub_prompt += f\"issue:{issue['issue']}\\n\"\n",
    "                    review_node.metadata['issues'].append(issue['issue'])\n",
    "                    \n",
    "\n",
    "            print(review_sub_prompt)\n",
    "            if not (review_sub_prompt == \"\"):\n",
    "                list_of_team_and_reviews = self.team_route_obj.route_text(review_sub_prompt).teams\n",
    "                for team_and_review in list_of_team_and_reviews:\n",
    "                    team_id = team_and_review.team\n",
    "                    team = self.team_route_obj.team_details[\"teams\"][team_id][\"teamName\"]\n",
    "                    print(\"...\",team,team_and_review)\n",
    "                    teams.append(team)\n",
    "                    review_node.metadata['assigned_teams'].append(team)\n",
    "                    review_node.metadata['reasons'].append(team_and_review.reason_for_assignment)\n",
    "            else:\n",
    "                print(\"No issues found in the review\")\n",
    "            review_nodes_assigned.append(review_node)\n",
    "            self.backup=review_nodes_assigned\n",
    "        return review_nodes_assigned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building route layers ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6it [00:00, 14742.72it/s]\n"
     ]
    }
   ],
   "source": [
    "team_route = TeamRoutePipeline(\n",
    "    team_details_file_path = './datas/team_details_Google_Pay_Secure_UPI_payment.json',\n",
    "    app_details_file_path = './datas/api_result_appdescr_Google_Pay_Secure_UPI_payment.json',\n",
    ")\n",
    "\n",
    "# # team_route.build()\n",
    "team_route.build_routes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "_review_route_pipeline = ReviewRoutePipeline(team_route)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _review_file_path = './datas/api_result_reviews_relv_Google_Pay_Secure_UPI_payment_v0.json'\n",
    "# _reviews = get_reviews_from_file(_review_file_path)\n",
    "# _review_nodes = convert_reviews_into_textnodes(_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_review_nodes_routed = _review_route_pipeline(_review_nodes[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM is explicitly disabled. Using MockLLM.\n",
      "Embeddings have been explicitly disabled. Using MockEmbedding.\n",
      "LLM is explicitly disabled. Using MockLLM.\n",
      "Embeddings have been explicitly disabled. Using MockEmbedding.\n"
     ]
    }
   ],
   "source": [
    "# review_vectara_engine = ReviewVectaraEngine(\n",
    "#     verbose=True, \n",
    "#     similarity_top_k=2,\n",
    "#     summary_enabled=False,\n",
    "#     summary_response_lang=\"eng\",\n",
    "#     summary_num_results=7,\n",
    "#     llm_model_name=\"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n",
    "# )\n",
    "\n",
    "# review_vectara_engine.ingest_nodes(_review_nodes_routed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store review nodes in the JSON file\n",
    "# ID and Metadata are stored in the JSON file\n",
    "# Load existing review nodes from the JSON file and append the new review nodes to it\n",
    "\n",
    "def store_review_nodes(review_nodes,file_path):\n",
    "    review_nodes_dict = [{'_id':review_node.dict()['id_'], 'metadata':review_node.dict()['metadata']} for review_node in review_nodes]\n",
    "    try:\n",
    "        with open(file_path,'r') as f:\n",
    "            existing_review_nodes = json.load(f)\n",
    "            existing_review_nodes.extend(review_nodes_dict)\n",
    "    except:\n",
    "        existing_review_nodes = review_nodes_dict\n",
    "    with open(file_path,'w') as f:\n",
    "        json.dump(existing_review_nodes,f,indent=4)\n",
    "\n",
    "store_review_nodes(_review_nodes_routed,'./datas/team_assigned_result_reviews_relv_Google_Pay_Secure_UPI_payment.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
