{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from common_imports import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "JSON_Engine is a wrapper over LlamaIndex JSON Query Engine\n",
    "It takes a JSON prompt and a pydanctic class name as input in the constructor\n",
    "It uses OpenAI API to generate the output\n",
    "\n",
    "run method processes the prompt and returns the output in the form of pydantic class object\n",
    "\"\"\"\n",
    "class JSON_Engine(BaseTool):\n",
    "    def __init__(self, prompt, class_name, llm_model_name: str = \"mistralai/Mixtral-8x7B-Instruct-v0.1\",temperature=0.1, api_key_name = \"TOGETHER_API_KEY\",parse=True,gemma_mode=False):\n",
    "        self.output_parser = PydanticOutputParser(class_name)\n",
    "        # self.llm = TogetherLLM(model=llm_model_name, api_key=os.environ[api_key_name], temperature=temperature)\n",
    "        self.llm = OpenAI(model=\"gpt-3.5-turbo\", temperature=temperature)\n",
    "        self.json_prompt_str = prompt\n",
    "        self.class_name = class_name\n",
    "        self.json_prompt_str = self.output_parser.format(self.json_prompt_str)\n",
    "        self.json_prompt_tmpl = PromptTemplate(self.json_prompt_str)\n",
    "        if parse:\n",
    "            self.p = QueryPipeline(chain=[self.json_prompt_tmpl, self.llm, self.output_parser], verbose=False)\n",
    "        else:\n",
    "            self.p = QueryPipeline(chain=[self.json_prompt_tmpl, self.llm], verbose=False)\n",
    "        self.gemma_mode = gemma_mode\n",
    "    \n",
    "    def run(self, **kwargs):\n",
    "        response = self.p.run(**kwargs)\n",
    "        if self.gemma_mode:\n",
    "            pattern = r'{.*}'\n",
    "            matches = re.findall(pattern, response.message.content, re.DOTALL)\n",
    "            largest_match = max(matches, key=len, default=None)\n",
    "            print(largest_match)\n",
    "            response = self.output_parser.parse(largest_match)\n",
    "        return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TicketSummary(BaseModel):\n",
    "    summary:str = Field(...,description=\n",
    "            \"\"\"\n",
    "            Summary of all technical details to be included in the ticket.\n",
    "            \"\"\")\n",
    "    \n",
    "    priority:str = Field(...,description=\n",
    "            \"\"\"\n",
    "            Priority of the ticket. \n",
    "            High - If the issue is critical and needs immediate attention.\n",
    "            Medium - If the issue is important but can be resolved within a day.\n",
    "            Low - If the issue is minor and can be resolved within a week.\n",
    "            \"\"\")\n",
    "\n",
    "class TicketSummaryEngine:\n",
    "    def __init__(self):\n",
    "        self.engine = JSON_Engine(\"\"\"\n",
    "        Given some issues raised by the user (between <<< and >>>), \n",
    "        <<<\n",
    "        {issues}\n",
    "        >>>\n",
    "        \n",
    "        Summary of the technical details to be included in the ticket:\n",
    "        \"\"\",class_name = TicketSummary,temperature=0.1)\n",
    "    \n",
    "    def __call__(self,**kwargs):\n",
    "        return self.engine.run(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Ticket Json-\n",
    "tickcet_id\n",
    "timestamp\n",
    "review_json_list\n",
    "content\n",
    "issue_category\n",
    "priority\n",
    "team_assigned\n",
    "no_of_users_reported\n",
    "resolved_status\n",
    "resolution_timestamp\n",
    "solution\n",
    "\"\"\"\n",
    "\n",
    "def aggregate_reviews_teamwise(team_assigned_path=\"datas/team_assigned_result_reviews_relv_Google_Pay_Secure_UPI_payment.json\"):\n",
    "    ticket_summary_engine = TicketSummaryEngine()\n",
    "    priority_map = {\"High\":3,\"Medium\":2,\"Low\":1}\n",
    "    with open(team_assigned_path) as f:\n",
    "        reviews = json.load(f)\n",
    "    \n",
    "    team_wise_reviews = {}\n",
    "    for review in reviews:\n",
    "        teams = review['metadata']['assigned_teams']\n",
    "        for team in teams:\n",
    "            if team not in team_wise_reviews:\n",
    "                team_wise_reviews[team] = []\n",
    "            team_wise_reviews[team].append(review)\n",
    "\n",
    "    # sort reviews by date\n",
    "    for team in team_wise_reviews:\n",
    "        team_wise_reviews[team] = sorted(team_wise_reviews[team], key=lambda x: x['metadata']['date_time'], reverse=True)\n",
    "\n",
    "    \n",
    "    # Group reviews taking 4 reviews at a time\n",
    "    for team in team_wise_reviews:\n",
    "        team_wise_reviews[team] = [team_wise_reviews[team][i:i+4] for i in range(0, len(team_wise_reviews[team]), 4)]\n",
    "    \n",
    "    # Team wise tickets\n",
    "    team_wise_tickets = {}\n",
    "    for team in team_wise_reviews:\n",
    "        team_wise_tickets[team] = []\n",
    "        for reviews in team_wise_reviews[team]:\n",
    "            ticket = {}\n",
    "            ticket['ticket_id'] = str(random.randint(100000,999999))\n",
    "            ticket['timestamp'] = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "            # remove reviews['metadata']['assigned_teams'] and 'reasons'\n",
    "            reviews = [{'metadata':{k:v for k,v in review['metadata'].items() if k not in ['assigned_teams','reasons']}} for review in reviews]\n",
    "            ticket['review_json_list'] = reviews\n",
    "            concat_reviews = \"\\n\".join([\", \".join(review['metadata']['issues']) for review in reviews])\n",
    "            ticket_engine_output = ticket_summary_engine(issues=concat_reviews)\n",
    "            priority = ticket_engine_output.priority\n",
    "            ticket['content'] = ticket_engine_output.summary\n",
    "            ticket['content'] = concat_reviews\n",
    "            # ticket['issue_category'] = list(set([issue.split(' Team')[0] for review in reviews for issue in review['metadata']['assigned_teams']]))\n",
    "            ticket['priority'] = priority_map[priority]\n",
    "            ticket['team_assigned'] = team\n",
    "            ticket['no_of_users_reported'] = len(reviews)\n",
    "            ticket['resolved_status'] = 0\n",
    "            ticket['resolution_timestamp'] = \"\"\n",
    "            ticket['solution'] = \"\"\n",
    "            team_wise_tickets[team].append(ticket)\n",
    "\n",
    "    return team_wise_tickets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_tickets_into_list(team_wise_tickets):\n",
    "    tickets_list = []\n",
    "    for team in team_wise_tickets:\n",
    "        for ticket in team_wise_tickets[team]:\n",
    "            tickets_list.append(ticket)\n",
    "    return tickets_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "team_wise_tickets = aggregate_reviews_teamwise()\n",
    "tickets_list = extract_tickets_into_list(team_wise_tickets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tickets_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same in tickets.json\n",
    "with open(\"datas/extracted_tickets.json\",\"w\") as f:\n",
    "    json.dump(tickets_list,f,indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JSONify "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
