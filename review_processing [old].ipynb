{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import phoenix as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import llama_index\n",
    "from llama_index.core import Document\n",
    "from llama_index.indices.managed.vectara import VectaraIndex,VectaraAutoRetriever\n",
    "from llama_index.core.schema import TextNode\n",
    "from llama_index.core.vector_stores import MetadataInfo, VectorStoreInfo\n",
    "from llama_index.core.indices.service_context import ServiceContext\n",
    "from llama_index.llms.together import TogetherLLM\n",
    "from semantic_router import Route,RouteLayer\n",
    "from semantic_router.encoders import OpenAIEncoder\n",
    "\n",
    "from common_imports import *\n",
    "import json , os , sys , time , re\n",
    "import dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.launch_app()\n",
    "llama_index.core.set_global_handler(\"arize_phoenix\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review Emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_reviews_into_textnodes(reviews):\n",
    "    # reviews : list of reviews \n",
    "    # review_documents = [Document(text=review['text'],metadata=review , doc_id=review['metadata']) for review in reviews]\n",
    "    review_nodes = [TextNode(text=review['text'],metadata={\n",
    "        'source':str(review['source']),\n",
    "        'date_time':str(review['metadata']['at']),\n",
    "        'reviewId':str(review['metadata']['reviewId']),\n",
    "        'userName':str(review['metadata']['userName']),\n",
    "        'rating':str(review['metadata']['score']),\n",
    "        'thumbsUpCount':str(review['metadata']['thumbsUpCount']),\n",
    "        'appVersion':str(review['metadata']['appVersion']),\n",
    "        'replyContent':str(review['metadata']['replyContent']),\n",
    "        'repliedAt':str(review['metadata']['repliedAt']),\n",
    "        'text':str(review['text']),\n",
    "        'app_name':str(review['app_name']),\n",
    "    },id_ = review['metadata']['reviewId']\n",
    "    ) for review in reviews]\n",
    "\n",
    "    return review_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reviews_from_file(file_path):\n",
    "    # file_path : path to the file containing reviews\n",
    "    # Returns list of reviews\n",
    "    with open(file_path,'r') as f:\n",
    "        reviews = json.load(f)\n",
    "    return reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReviewEngine:\n",
    "    def __init__(\n",
    "            self,\n",
    "            verbose=True, \n",
    "            similarity_top_k=2,\n",
    "            summary_enabled=False,\n",
    "            summary_response_lang=\"eng\",\n",
    "            summary_num_results=7,\n",
    "            llm_model_name=\"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n",
    "            ):\n",
    "        self.index = VectaraIndex(show_progress=True)\n",
    "        self.vector_store_info = VectorStoreInfo(\n",
    "            content_info = \"App reviews from different sources\",\n",
    "            metadata_info = [\n",
    "                MetadataInfo(\n",
    "                    name = \"source\",\n",
    "                    type = \"string\",\n",
    "                    description=\"Source of the review like playstore,appstore etc\"\n",
    "                ),\n",
    "                MetadataInfo(\n",
    "                    name = \"date_time\",\n",
    "                    type = \"string\",\n",
    "                    description=\"Date and time of the review\"\n",
    "                ),\n",
    "                MetadataInfo(\n",
    "                    name = \"reviewId\",\n",
    "                    type = \"string\",\n",
    "                    description=\"Review Id\"\n",
    "                ),\n",
    "                MetadataInfo(\n",
    "                    name = \"userName\",\n",
    "                    type = \"string\",\n",
    "                    description=\"User name of the reviewer\"\n",
    "                ),\n",
    "                MetadataInfo(\n",
    "                    name = \"rating\",\n",
    "                    type = \"float\",\n",
    "                    description=\"Rating given by the reviewer\"\n",
    "                ),\n",
    "                MetadataInfo(\n",
    "                    name = \"thumbsUpCount\",\n",
    "                    type = \"int\",\n",
    "                    description=\"Number of thumbs up i.e. the relevance of the review\"\n",
    "                ),\n",
    "                MetadataInfo(\n",
    "                    name = \"appVersion\",\n",
    "                    type = \"string\",\n",
    "                    description=\"App version of the app for which review is given\"\n",
    "                ),\n",
    "                MetadataInfo(\n",
    "                    name = \"replyContent\",\n",
    "                    type = \"string\",\n",
    "                    description=\"Reply content to the review by any other user\"\n",
    "                ),\n",
    "                MetadataInfo(\n",
    "                    name = \"repliedAt\",\n",
    "                    type = \"string\",\n",
    "                    description=\"Date and time of the reply\"\n",
    "                ),\n",
    "                MetadataInfo(\n",
    "                    name = \"app_name\",\n",
    "                    type = \"string\",\n",
    "                    description=\"App name for which review is given\"\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "        self.llm = TogetherLLM(\n",
    "            model=llm_model_name, api_key=os.environ['TOGETHER_API_KEY']\n",
    "        )\n",
    "\n",
    "        self.verbose = verbose\n",
    "        self.similarity_top_k = similarity_top_k\n",
    "        self.summary_enabled = summary_enabled\n",
    "        self.summary_response_lang = summary_response_lang\n",
    "        self.summary_num_results = summary_num_results\n",
    "\n",
    "        self.build()\n",
    "    \n",
    "    def build(self):\n",
    "        self.auto_retriever = VectaraAutoRetriever(\n",
    "            vector_store_info=self.vector_store_info,\n",
    "            llm=self.llm,\n",
    "            index=self.index,\n",
    "            show_progress=True,\n",
    "            summary_enabled = self.summary_enabled,\n",
    "            summary_response_lang = self.summary_response_lang,\n",
    "            summary_num_results = self.summary_num_results,\n",
    "            verbose=self.verbose,\n",
    "        )\n",
    "        self.retriever = self.index.as_retriever(\n",
    "            similarity_top_k=self.similarity_top_k,\n",
    "            summary_enabled = self.summary_enabled,\n",
    "            summary_response_lang = self.summary_response_lang,\n",
    "            summary_num_results = self.summary_num_results,\n",
    "            llm = self.llm,\n",
    "            )\n",
    "        self.query_engine = self.index.as_query_engine(similarity_top_k=5)\n",
    "\n",
    "    def ingest_reviews(self,review_file_path,start=None,end=None):\n",
    "        self.reviews = get_reviews_from_file(review_file_path)[start:end]\n",
    "        self.review_nodes = convert_reviews_into_textnodes(self.reviews)\n",
    "        self.index = VectaraIndex(nodes = self.review_nodes, show_progress=True)\n",
    "        self.build()\n",
    "\n",
    "    def ingest_nodes(self,nodes):\n",
    "        self.index = VectaraIndex(nodes = nodes, show_progress=True)\n",
    "        self.build()\n",
    "\n",
    "    def run(self,query , mode:str):\n",
    "        # query : query string\n",
    "        # mode : 'autoretriever' or 'retriever' or 'query_engine'\n",
    "        if mode == 'autoretriever':\n",
    "            return self.auto_retriever.retrieve(query)\n",
    "        elif mode == 'retriever':\n",
    "            return self.retriever.retrieve(query)\n",
    "        elif mode == 'query_engine':\n",
    "            return self.query_engine.query(query)\n",
    "        else:\n",
    "            return \"Invalid mode\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_review_file_path = './datas/api_result_reviews_relv_Google_Pay_Secure_UPI_payment_v0.json'\n",
    "_reviews = get_reviews_from_file(_review_file_path)\n",
    "_review_nodes = convert_reviews_into_textnodes(_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_review_nodes[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_engine = ReviewEngine(\n",
    "    verbose=True, \n",
    "    similarity_top_k=2,\n",
    "    summary_enabled=False,\n",
    "    summary_response_lang=\"eng\",\n",
    "    summary_num_results=7,\n",
    "    llm_model_name=\"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_file_path = './datas/api_result_reviews_relv_Grand_Theft_Auto_San_Andreas_v0.json'\n",
    "review_engine.ingest_reviews(review_file_path,end=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans = review_engine.run('What are the reviews from playstore',mode='autoretriever')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_retriever = VectaraAutoRetriever(\n",
    "    review_engine.index,\n",
    "    vector_store_info=review_engine.vector_store_info,\n",
    "    llm=review_engine.llm,\n",
    "    filter=\"doc.source == 'playstore'\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_retriever.retrieve(\"What are the reviews\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generic JSON Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "JSON_Engine is a wrapper over LlamaIndex JSON Query Engine\n",
    "It takes a JSON prompt and a pydanctic class name as input in the constructor\n",
    "It uses OpenAI API to generate the output\n",
    "\n",
    "run method processes the prompt and returns the output in the form of pydantic class object\n",
    "\"\"\"\n",
    "class JSON_Engine(BaseTool):\n",
    "    def __init__(self, prompt, class_name, llm_model_name: str = \"mistralai/Mixtral-8x7B-Instruct-v0.1\",temperature=0.1, api_key_name = \"TOGETHER_API_KEY\",parse=True):\n",
    "        self.output_parser = PydanticOutputParser(class_name)\n",
    "        self.llm = TogetherLLM(model=llm_model_name, api_key=os.environ[api_key_name], temperature=temperature)\n",
    "        # self.llm = OpenAI(model=\"gpt-3.5-turbo\", temperature=temperature)\n",
    "        self.json_prompt_str = prompt\n",
    "        self.class_name = class_name\n",
    "        self.json_prompt_str = self.output_parser.format(self.json_prompt_str)\n",
    "        self.json_prompt_tmpl = PromptTemplate(self.json_prompt_str)\n",
    "        if parse:\n",
    "            self.p = QueryPipeline(chain=[self.json_prompt_tmpl, self.llm, self.output_parser], verbose=False)\n",
    "        else:\n",
    "            self.p = QueryPipeline(chain=[self.json_prompt_tmpl, self.llm], verbose=False)\n",
    "    \n",
    "    def run(self, **kwargs):\n",
    "        response = self.p.run(**kwargs)\n",
    "        return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Ingestion Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SubContext(BaseModel):\n",
    "    subContext:str = Field(...,description=\n",
    "                        \"\"\"\n",
    "                        Subset of a large review context which is relevant to the query. \n",
    "                        It should be a full sentence containing a single aspect of the review.\n",
    "                        \"\"\")\n",
    "    sentiment:str = Field(...,description=\n",
    "                           \"\"\"\n",
    "                             Sentiment of the review i.e. positive or negative\n",
    "                            'positive' means the review has positive sentiment\n",
    "                            'negative' means the review has negative sentiment\n",
    "                            \"\"\")\n",
    "    containsTechnicalTerms:bool = Field(...,description=\n",
    "                                        \"\"\"\n",
    "                                        Whether the sub context contains technical terms or not such as any glitch, bug, feature etc.\n",
    "                                        True means the sub context contains technical terms\n",
    "                                        False means the sub context does not contain technical terms\n",
    "                                        \"\"\")\n",
    "    \n",
    "\n",
    "class ListOfSubContext(BaseModel):\n",
    "    subContexts:List[SubContext] = Field(...,description=\n",
    "            \"\"\"\n",
    "            List of sub contexts extracted from the review context\n",
    "            \"\"\")\n",
    "\n",
    "class SubContextEngine:\n",
    "    def __init__(self):\n",
    "        self.engine = JSON_Engine(\"\"\"\n",
    "        Given a review context, extract the list of relevant sub contexts (between <<< and >>>):\n",
    "        <<<\n",
    "        {text}\n",
    "        >>>\n",
    "\n",
    "        Note: \n",
    "        These sub contexts should be all different aspects of the review context and should be full sentences. \n",
    "        Also provide the sentiment of each sub context and whether it contains any technical terms or not.\n",
    "        \"\"\",class_name = ListOfSubContext,temperature=0.1)\n",
    "    \n",
    "    def __call__(self,**kwargs):\n",
    "        return self.engine.run(**kwargs)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReviewDegenAndSentimentPipeline:\n",
    "    def __init__(self):\n",
    "        self.sub_context_engine = SubContextEngine()\n",
    "    \n",
    "    def __call__(self,review):\n",
    "        sub_contexts = self.sub_context_engine(text=review)\n",
    "        sub_contexts = sub_contexts.dict()\n",
    "        return sub_contexts['subContexts']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "_sub_contexts = ReviewDegenAndSentimentPipeline()(review=\"\"\"\n",
    "It's good for a while but the bugs makes the experience very difficult. You will have to repeatedly reconfigure the sizes of on-screen buttons. Many missions, especially the longer ones may not have checkpoints, and if failed, one has to do them all over again. Flying in general is really difficult, for joystick is the only available option when flying a vehicle. All in all, buy this game for nostalgia, smash your phone afterwards for the bugs which won't ever be fixed.\n",
    "                                               \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'subContext': \"It's good for a while but the bugs makes the experience very difficult.\",\n",
       "  'sentiment': 'negative',\n",
       "  'containsTechnicalTerms': True},\n",
       " {'subContext': 'You will have to repeatedly reconfigure the sizes of on-screen buttons.',\n",
       "  'sentiment': 'negative',\n",
       "  'containsTechnicalTerms': True},\n",
       " {'subContext': 'Many missions, especially the longer ones may not have checkpoints, and if failed, one has to do them all over again.',\n",
       "  'sentiment': 'negative',\n",
       "  'containsTechnicalTerms': False},\n",
       " {'subContext': 'Flying in general is really difficult, for joystick is the only available option when flying a vehicle.',\n",
       "  'sentiment': 'negative',\n",
       "  'containsTechnicalTerms': True},\n",
       " {'subContext': \"Buy this game for nostalgia, smash your phone afterwards for the bugs which won't ever be fixed.\",\n",
       "  'sentiment': 'negative',\n",
       "  'containsTechnicalTerms': True}]"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_sub_contexts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Routes for different team"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UtterenceList(BaseModel):\n",
    "    utterenceList : List[str] = Field(...,description=\"\"\"List of different utterances / use case related to the given team's work\"\"\")\n",
    "\n",
    "class UtterenceEngine:\n",
    "    def __init__(self):\n",
    "        self.engine = JSON_Engine(\"\"\"\n",
    "        Given an app description (between ### and ###):\n",
    "        ###\n",
    "        {app_description}\n",
    "        ###\n",
    "\n",
    "\n",
    "        Given a team's scope of work (between <<< and >>>):\n",
    "        <<<\n",
    "        {scopes}\n",
    "        >>>\n",
    "\n",
    "        List down the more than 15 different utterances / use cases related to the given team's work and the app_description where they work.\n",
    "        These utterances should be full sentences and refers to the different bugs, features, improvements, etc. related to the team's work.\n",
    "        Strictly stick to the scope of work of the team.\n",
    "        \"\"\",class_name = UtterenceList,temperature=0.1)\n",
    "    \n",
    "    def __call__(self,**kwargs):\n",
    "        return self.engine.run(**kwargs)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TechnicalSummary(BaseModel):\n",
    "    summary:str = Field(...,description=\n",
    "            \"\"\"\n",
    "            Summary of all technical details associated with the app\n",
    "            \"\"\")\n",
    "\n",
    "class TechnicalSummaryEngine:\n",
    "    def __init__(self):\n",
    "        self.engine = JSON_Engine(\"\"\"\n",
    "        Given a raw app description (between <<< and >>>):\n",
    "        <<<\n",
    "        {app_description}\n",
    "        >>>\n",
    "        \n",
    "        Summarize all the technical details associated with the app into a brief summary. Only include techincal features. \n",
    "        \"\"\",class_name = TechnicalSummary,temperature=0.1)\n",
    "    \n",
    "    def __call__(self,**kwargs):\n",
    "        return self.engine.run(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TeamRoutePipeline:\n",
    "    def __init__(self, team_details_file_path:str, app_details_file_path:str,app_description=None,build_mode=False,score_threshold=0.5):\n",
    "        self.team_details_file_path = team_details_file_path\n",
    "        # load app details from the file\n",
    "        with open(app_details_file_path,'r') as f:\n",
    "            app_details = json.load(f)\n",
    "        with open(self.team_details_file_path,'r') as f:\n",
    "            self.team_details = json.load(f)\n",
    "\n",
    "        self.app_name = app_details[\"title\"]\n",
    "        self.app_description = app_description\n",
    "        self.utterance_engine = UtterenceEngine()\n",
    "        self.technical_summary_engine = TechnicalSummaryEngine()\n",
    "\n",
    "        self.routes = None\n",
    "        self.route_layer = None\n",
    "        self.routing_encoder = None\n",
    "        self.score_threshold = score_threshold\n",
    "\n",
    "        if app_description is None:\n",
    "            self.app_description = app_details[\"description\"]\n",
    "            self.summarized_app_description = self.technical_summary_engine(app_description=self.app_description).summary\n",
    "        if build_mode:\n",
    "            self.build()\n",
    "\n",
    "    def build(self):\n",
    "        # load team details from the file\n",
    "        with open(self.team_details_file_path,'r') as f:\n",
    "            self.team_details = json.load(f)\n",
    "        print(\"Building utterances ...\")\n",
    "        for i,team in tqdm(enumerate(self.team_details[\"teams\"])):\n",
    "            utterance_ans = self.utterance_engine(app_description=self.summarized_app_description,scopes=str(team[\"scopes\"])).utterenceList\n",
    "            self.team_details[\"teams\"][i][\"utterances\"] = utterance_ans\n",
    "        # save the updated team details to the file\n",
    "        with open(self.team_details_file_path,'w') as f:\n",
    "            json.dump(self.team_details,f,indent=4)  \n",
    "\n",
    "    def build_routes(self):\n",
    "        with open(self.team_details_file_path,'r') as f:\n",
    "            self.team_details = json.load(f)\n",
    "        self.routes = []\n",
    "        print(\"Building route layers ...\")\n",
    "        for i,team in tqdm(enumerate(self.team_details[\"teams\"])):\n",
    "            team_name = team[\"teamName\"]\n",
    "            try:\n",
    "                team_utterances = team[\"utterances\"]\n",
    "            except:\n",
    "                print(f\"team_name : {team_name} has not utterances ... build_routes paused\")\n",
    "                return\n",
    "\n",
    "            route = Route(name=team_name,utterances=team_utterances,score_threshold=self.score_threshold)\n",
    "            self.routes.append(route)\n",
    "        self.routing_encoder = OpenAIEncoder()\n",
    "        self.route_layer = RouteLayer(encoder=self.routing_encoder, routes=self.routes)\n",
    "\n",
    "    def route_text(self, text):\n",
    "        if self.route_layer is None:\n",
    "            print(\"Routes are not built yet\")\n",
    "            return None\n",
    "        return self.route_layer(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReviewRoutePipeline:\n",
    "    def __init__(self, team_route_obj):\n",
    "        self.review_degen_pipeline = ReviewDegenAndSentimentPipeline()\n",
    "        self.team_route_obj = team_route_obj\n",
    "\n",
    "    def __call__(self,review_nodes):\n",
    "        review_nodes_assigned = []\n",
    "        for review_node in review_nodes:\n",
    "            subsets_with_sentiment = self.review_degen_pipeline(review_node.text)\n",
    "            # assign each subset to a team\n",
    "            # Loop over subsets_with_sentiment \n",
    "            teams = []\n",
    "            for subset_with_sentiment in subsets_with_sentiment:\n",
    "                print(\">>> \",subset_with_sentiment['subContext'])\n",
    "                if subset_with_sentiment[\"sentiment\"] == \"negative\" and subset_with_sentiment[\"containsTechnicalTerms\"]:\n",
    "                    team_name = self.team_route_obj.route_text(subset_with_sentiment['subContext']).name\n",
    "                    team_name = team_name if team_name else \"General\"\n",
    "                    print(f\"subset_with_sentiment : {subset_with_sentiment} assigned to Team: {team_name}\")\n",
    "                    teams.append(team_name)\n",
    "            review_node.metadata['assigned_teams'] = teams\n",
    "            review_node.metadata['positive_subsets'] = [subset_with_sentiment['subContext'] for subset_with_sentiment in subsets_with_sentiment if subset_with_sentiment[\"sentiment\"] == \"positive\"]\n",
    "            review_node.metadata['negative_subsets'] = [subset_with_sentiment['subContext'] for subset_with_sentiment in subsets_with_sentiment if subset_with_sentiment[\"sentiment\"] == \"negative\" and subset_with_sentiment[\"containsTechnicalTerms\"]]\n",
    "            review_node.metadata['subsets_with_sentiment'] = subsets_with_sentiment\n",
    "            review_nodes_assigned.append(review_node)\n",
    "        return review_nodes_assigned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "team_route = TeamRoutePipeline(\n",
    "    team_details_file_path = './datas/team_details_Google_Pay_Secure_UPI_payment.json',\n",
    "    app_details_file_path = './datas/api_result_appdescr_Google_Pay_Secure_UPI_payment.json',\n",
    "    score_threshold=0.5\n",
    ")\n",
    "\n",
    "# team_route.build()\n",
    "# team_route.build_routes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "team_route.route_layer.get_thresholds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_review_route_pipeline = ReviewRoutePipeline(team_route)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_review_nodes_routed = _review_route_pipeline(_review_nodes[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
